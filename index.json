[{"authors":["zhangzs"],"categories":null,"content":"Hi! My name is Zhuosheng Zhang (张倬胜 in Chinese), or you can call me Coulson, which sounds very similar to my Chinese name. I am pursuing a Ph.D. in the Department of Computer Science and Engineering  at Shanghai Jiao Tong University, where I am advised by Prof. Hai Zhao and work in the BCMI-NLP Group.\tI obtained my Master’s degree from Shanghai Jiao Tong University in March 2020. Prior to that, in 2016, I received my Bachelor’s degree from Wuhan University, advised by Haojun Ai.\nFrom 06/2019 - now, I am an internship research fellow at NICT, working with Rui Wang, Kehai Chen, Masao Utiyama, and Eiichiro Sumita.\nResearch Interest: Natural Language Understanding in general, with enthusiasm to plug human expertise into the methodology of language learning, and repurpose off-the-shelf algorithms for training machines to read, comprehend, and reason over natural language texts. The methodology is built upon the principled theme of Machine Reading Comprehension, as a major topic of artificial intelligence, in which I focus on a variety of methods involving structured knowledge contextualization (e.g., semantics, syntax, commonsense, etc.) (AAAI\u0026rsquo;20, AAAI\u0026rsquo;20b), language unit representation and vocabulary learning\r(ACL\u0026rsquo;19,\rTASLP,\rCOLING\u0026rsquo;18),\rand multimodal perception\r(ICLR\u0026rsquo;20). The approaches enable effective, efficient, and interpretable solutions for real-world applications, such as question answering\r(COLING\u0026rsquo;18b), dialogue and interactive systems\r(COLING\u0026rsquo;18c, COLING\u0026rsquo;18d), and machine translation. See my publications.\rOpen Source: The sources of my research projects are available at Github @cooelf. Some featured projects are: \rDUA : a deep utterance aggregation model for conversational response selection task.\rSemBERT : a semantics-aware language understanding toolkit.\rAwesomeMRC: an overview of MRC research, and our SOTA implementation for the SQuAD2.0 Challenge.\rI am also glad to share my studies publicly. See my technical articles: SemBERT, Retro-Reader (in Chinese). Contact me by email (zhangzs [AT] sjtu.edu.cn).\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://cooelf.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Hi! My name is Zhuosheng Zhang (张倬胜 in Chinese), or you can call me Coulson, which sounds very similar to my Chinese name. I am pursuing a Ph.D. in the Department of Computer Science and Engineering  at Shanghai Jiao Tong University, where I am advised by Prof. Hai Zhao and work in the BCMI-NLP Group.\tI obtained my Master’s degree from Shanghai Jiao Tong University in March 2020. Prior to that, in 2016, I received my Bachelor’s degree from Wuhan University, advised by Haojun Ai.","tags":null,"title":"Zhuosheng Zhang","type":"authors"},{"authors":["Zuchao Li","Rui Wang","Kehai Chen","Masso Utiyama","Eiichiro Sumita","**Zhuosheng Zhang**","Hai Zhao"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"cc014866e3eafea1879c16d1bf962de9","permalink":"https://cooelf.github.io/publication/content/publication/li-2020-data/","publishdate":"2020-02-08T07:10:09.416357Z","relpermalink":"/publication/content/publication/li-2020-data/","section":"publication","summary":"","tags":null,"title":"Data-dependent Gaussian Prior Objective for Language Generation","type":"publication"},{"authors":["**Zhuosheng Zhang**","Kehai Chen","Rui Wang","Masao Utiyama","Eiichiro Sumita","Zuchao Li","Hai Zhao"],"categories":null,"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585699200,"objectID":"3602f1c7099b2e3fdb5f857da4f2bf0a","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2020-neural/","publishdate":"2020-02-08T07:10:09.416357Z","relpermalink":"/publication/content/publication/zhang-2020-neural/","section":"publication","summary":"Though visual information has been introduced for enhancing neural machine translation (NMT), its effectiveness strongly relies on the availability of large amounts of bilingual parallel sentence pairs with manual image annotations. In this paper, we present a universal visual representation learned over the monolingual corpora with image annotations, which overcomes the lack of large-scale bilingual sentence-image pairs, thereby extending image applicability in NMT. In detail, a group of images with similar topics to the source sentence will be retrieved from a light topic-image lookup table learned over the existing sentence-image pairs, and then is encoded as image representations by a pre-trained ResNet. An attention layer with a gated weighting is to fuse the visual information and text information as input to the decoder for predicting target translations. In particular, the proposed method enables the visual information to be integrated into large-scale text-only NMT in addition to the multimodel NMT. Experiments on four widely used translation datasets, including the WMT'16 English-to-Romanian, WMT'14 English-to-German, WMT'14 English-to-French, and Multi30K, show that the proposed approach achieves significant improvements over strong baselines. ","tags":null,"title":"Neural Machine Translation with Universal Visual Representation","type":"publication"},{"authors":["Shuailiang Zhang","Hai Zhao","Yuwei Wu","**Zhuosheng Zhang**","Xi Zhou","Xiang Zhou"],"categories":null,"content":"","date":1581120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1581120000,"objectID":"374b7baefad52c9100407d7097f7d0e5","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2020-dcmn/","publishdate":"2020-02-08T00:00:00Z","relpermalink":"/publication/content/publication/zhang-2020-dcmn/","section":"publication","summary":"","tags":null,"title":"DCMN+: Dual co-matching network for multi-choice reading comprehension","type":"publication"},{"authors":["Zuchao Li","Rui Wang","Kehai Chen","Masao Utiyama","Eiichiro Sumita","**Zhuosheng Zhang**","Hai Zhao"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"73ce00ed420b02ac2a117037be2b5a24","permalink":"https://cooelf.github.io/publication/content/publication/li-2020-explicit/","publishdate":"2020-02-08T08:19:23.96401Z","relpermalink":"/publication/content/publication/li-2020-explicit/","section":"publication","summary":"","tags":null,"title":"Explicit Sentence Compression for Neural Machine Translation","type":"publication"},{"authors":["**Zhuosheng Zhang**","Yuwei Wu","Hai Zhao","Zuchao Li","Shuailiang Zhang","Xi Zhou","Xiang Zhou"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"4a7221c6e8a2d83c11b16c1fc6543db1","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2020-sem-bert/","publishdate":"2020-02-08T07:10:09.419359Z","relpermalink":"/publication/content/publication/zhang-2020-sem-bert/","section":"publication","summary":"The latest work on language representations carefully integrates contextualized features into language model training, which enables a series of success especially in various machine reading comprehension and natural language inference tasks. However, the existing language representation models including ELMo, GPT and BERT only exploit plain context-sensitive features such as character or word embeddings. They rarely consider incorporating structured semantic information which can provide rich semantics for language representation. To promote natural language understanding, we propose to incorporate explicit contextual semantics from pre-trained semantic role labeling, and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics over a BERT backbone. SemBERT keeps the convenient usability of its BERT precursor in a light fine-tuning way without substantial task-specific modifications. Compared with BERT, semantics-aware BERT is as simple in concept but more powerful. It obtains new state-of-the-art or substantially improves results on ten reading comprehension and language inference tasks. ","tags":null,"title":"Semantics-aware BERT for language understanding","type":"publication"},{"authors":["**Zhuosheng Zhang**","Yuwei Wu","Junru Zhou","Sufeng Duan","Hai Zhao","Rui Wang"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"3dabb18788f6454187795a657bdc4f4f","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2019-sg/","publishdate":"2020-02-08T07:10:09.422322Z","relpermalink":"/publication/content/publication/zhang-2019-sg/","section":"publication","summary":"For machine reading comprehension, the capacity of effectively modeling the linguistic knowledge from the detail-riddled and lengthy passages and getting ride of the noises is essential to improve its performance. Traditional attentive models attend to all words without explicit constraint, which results in inaccurate concentration on some dispensable words. In this work, we propose using syntax to guide the text modeling by incorporating explicit syntactic constraints into attention mechanism for better linguistically motivated word representations. In detail, for self-attention network (SAN) sponsored Transformer-based encoder, we introduce syntactic dependency of interest (SDOI) design into the SAN to form an SDOI-SAN with syntax-guided self-attention. Syntax-guided network (SG-Net) is then composed of this extra SDOI-SAN and the SAN from the original Transformer encoder through a dual contextual architecture for better linguistics inspired representation. To verify its effectiveness, the proposed SG-Net is applied to typical pre-trained language model BERT which is right based on a Transformer encoder. Extensive experiments on popular benchmarks including SQuAD 2.0 and RACE show that the proposed SG-Net design helps achieve substantial performance improvement over strong baselines. ","tags":null,"title":"SG-Net: Syntax-Guided Machine Reading Comprehension","type":"publication"},{"authors":["Zuchao Li","Shexia He","Hai Zhao","Yiqing Zhang","**Zhuosheng Zhang**","Xi Zhou","Xiang Zhou"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"60b0e9abe8764f97dc89eb3e43e43338","permalink":"https://cooelf.github.io/publication/content/publication/li-2019-dependency/","publishdate":"2020-02-08T08:19:23.971013Z","relpermalink":"/publication/content/publication/li-2019-dependency/","section":"publication","summary":"","tags":null,"title":"Dependency or span, end-to-end uniform semantic role labeling","type":"publication"},{"authors":["**Zhuosheng Zhang**","Hai Zhao","Kangwei Ling","Jiangtong Li","Shexia He","Guohong Fu"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"407f97c92a14df6d001a21e539f63eeb","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2018-subword/","publishdate":"2020-02-08T07:10:09.426319Z","relpermalink":"/publication/content/publication/zhang-2018-subword/","section":"publication","summary":"Representation learning is the foundation of machine reading comprehension and inference. In state-of-the-art models, character-level representations have been broadly adopted to alleviate the problem of effectively representing rare or complex words. However, character itself is not a natural minimal linguistic unit for representation or word embedding composing due to ignoring the linguistic coherence of consecutive characters inside word. This paper presents a general subword-augmented embedding framework for learning and composing computationally-derived subword-level representations. We survey a series of unsupervised segmentation methods for subword acquisition and different subword-augmented strategies for text understanding, showing that subword-augmented embedding significantly improves our baselines in various types of text understanding tasks on both English and Chinese benchmarks.","tags":null,"title":"Effective Subword Segmentation for Text Comprehension","type":"publication"},{"authors":["**Zhuosheng Zhang**","Yuwei Wu","Zuchao Li","Hai Zhao"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"5e127bd05bc0f61b9f7d3d9bd0c04533","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2019-explicit/","publishdate":"2020-02-08T07:10:09.427318Z","relpermalink":"/publication/content/publication/zhang-2019-explicit/","section":"publication","summary":"Who did what to whom is a major focus in natural language understanding, which is right the aim of semantic role labeling (SRL) task. Despite of sharing a lot of processing characteristics and even task purpose, it is surprisingly that jointly considering these two related tasks was never formally reported in previous work. Thus this paper makes the first attempt to let SRL enhance text comprehension and inference through specifying verbal predicates and their corresponding semantic roles. In terms of deep learning models, our embeddings are enhanced by explicit contextual semantic role labels for more fine-grained semantics. We show that the salient labels can be conveniently added to existing models and significantly improve deep learning models in challenging text comprehension tasks. Extensive experiments on benchmark machine reading comprehension and inference datasets verify that the proposed semantic learning helps our system reach new state-of-the-art over strong baselines which have been enhanced by well pretrained language models from the latest progress. ","tags":null,"title":"Explicit Contextual Semantics for Text Comprehension","type":"publication"},{"authors":["**Zhuosheng Zhang**","Yafang Huang","Hai Zhao"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3f101fb60a16c8770b04f22c43e65bf0","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2019-acl/","publishdate":"2020-02-08T07:10:09.424319Z","relpermalink":"/publication/content/publication/zhang-2019-acl/","section":"publication","summary":"Pinyin-to-character (P2C) conversion is the core component of pinyin-based Chinese input method engine (IME). However, the conversion is seriously compromised by the ambiguities of Chinese characters corresponding to pinyin as well as the predefined fixed vocabularies. To alleviate such inconveniences, we propose a neural P2C conversion model augmented by a large online updating vocabulary with a target vocabulary sampling mechanism to support an open vocabulary learning during IME working. Our experiments show that the proposed approach reduces the decoding time on CPUs up to 50% on P2C tasks at the same or only negligible change in conversion accuracy, and the online updated vocabulary indeed helps our IME effectively follows user inputting behavior. ","tags":null,"title":"Open Vocabulary Learning for Neural Chinese Pinyin IME","type":"publication"},{"authors":["Zuchao Li","Hai Zhao","**Zhuosheng Zhang**","Rui Wang","Masao Utiyama","Eiichiro Sumita"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"373e5aea0f58a75d42383324e3498986","permalink":"https://cooelf.github.io/publication/content/publication/li-2019-sjtu/","publishdate":"2020-02-08T08:19:23.96801Z","relpermalink":"/publication/content/publication/li-2019-sjtu/","section":"publication","summary":"","tags":null,"title":"SJTU-NICT at MRP 2019: Multi-Task Learning for End-to-End Uniform Semantic Graph Parsing","type":"publication"},{"authors":["Zuchao Li","Shexia He","Jiaxun Cai","**Zhuosheng Zhang**","Hai Zhao","Gongshen Liu","Linlin Li","Luo Si"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"4ab208285a097eb9dff95b3ec57bedb7","permalink":"https://cooelf.github.io/publication/content/publication/li-2018-unified/","publishdate":"2020-02-08T08:19:23.97901Z","relpermalink":"/publication/content/publication/li-2018-unified/","section":"publication","summary":"","tags":null,"title":"A unified syntax-aware framework for semantic role labeling","type":"publication"},{"authors":["**Zhuosheng Zhang**","Yafang Huang","Pengfei Zhu","Hai Zhao"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"3e40b33c03727f015591211d0bacba97","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2018-effective/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/content/publication/zhang-2018-effective/","section":"publication","summary":"","tags":null,"title":"Effective character-augmented word embedding for machine reading comprehension","type":"publication"},{"authors":["Zuchao Li","Shexia He","**Zhuosheng Zhang**","Hai Zhao"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"96b7cbf6adc9f39b4eac67ac37c3f2c4","permalink":"https://cooelf.github.io/publication/content/publication/li-2018-joint/","publishdate":"2020-02-08T08:19:23.987009Z","relpermalink":"/publication/content/publication/li-2018-joint/","section":"publication","summary":"","tags":null,"title":"Joint learning of pos and dependencies for multilingual universal dependency parsing","type":"publication"},{"authors":["Pengfei Zhu","**Zhuosheng Zhang**","Jiangtong Li","Yafang Huang","Hai Zhao"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"921758626e73a42499c67a2b3e07ea57","permalink":"https://cooelf.github.io/publication/content/publication/zhu-2018-lingke/","publishdate":"2020-02-08T07:10:09.437318Z","relpermalink":"/publication/content/publication/zhu-2018-lingke/","section":"publication","summary":"Traditional chatbots usually need a mass of human dialogue data, especially when using supervised machine learning method. Though they can easily deal with single-turn question answering, for multi-turn the performance is usually unsatisfactory. In this paper, we present Lingke, an information retrieval augmented chatbot which is able to answer questions based on given product introduction document and deal with multi-turn conversations. We will introduce a fine-grained pipeline processing to distill responses based on unstructured documents, and attentive sequential context-response matching for multi-turn conversations. ","tags":null,"title":"Lingke: A Fine-grained Multi-turn Chatbot for Customer Service","type":"publication"},{"authors":["**Zhuosheng Zhang**","Jiangtong Li","Pengfei Zhu","Hai Zhao"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"9f763b2bc1c926f7094b0762d798addd","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2018-dua/","publishdate":"2020-02-08T07:10:09.430323Z","relpermalink":"/publication/content/publication/zhang-2018-dua/","section":"publication","summary":"Multi-turn conversation understanding is a major challenge for building intelligent dialogue systems. This work focuses on retrieval-based response matching for multi-turn conversation whose related work simply concatenates the conversation utterances, ignoring the interactions among previous utterances for context modeling. In this paper, we formulate previous utterances into context using a proposed deep utterance aggregation model to form a fine-grained context representation. In detail, a self-matching attention is first introduced to route the vital information in each utterance. Then the model matches a response with each refined utterance and the final matching score is obtained after attentive turns aggregation. Experimental results show our model outperforms the state-of-the-art methods on three multi-turn conversation benchmarks, including a newly introduced e-commerce dialogue corpus. ","tags":null,"title":"Modeling Multi-turn Conversation with Deep Utterance Aggregation","type":"publication"},{"authors":["Yafang Huang","Zuchao Li","**Zhuosheng Zhang**","Hai Zhao"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"da2dcae7a78b4b6509ca88218d112475","permalink":"https://cooelf.github.io/publication/content/publication/huang-2018-moon/","publishdate":"2020-02-08T07:10:09.439319Z","relpermalink":"/publication/content/publication/huang-2018-moon/","section":"publication","summary":"Chinese pinyin input method engine(IME) lets user conveniently input Chinese into a computer by typing pinyin through the common keyboard. In addition to offering high conversion quality, modern pinyin IME is supposed to aid user input with extended association function. However, existing solutions for such functions are roughly based on oversimplified matching algorithms at word-level, whose resulting products provide limited extension associated with user inputs. This work presents the Moon IME, a pinyin IME that integrates the attention-based neural machine translation (NMT) model and Information Retrieval (IR) to offer amusive and customizable association ability. The released IME is implemented on Windows via text services framework.","tags":null,"title":"Moon IME: Neural-based Chinese Pinyin Aided Input Method with Customizable Association","type":"publication"},{"authors":["**Zhuosheng Zhang**","Hai Zhao"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"69f41269fb0a080fdeb2baf8c2b5851c","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2018-gaokao/","publishdate":"2020-02-08T07:10:09.435319Z","relpermalink":"/publication/content/publication/zhang-2018-gaokao/","section":"publication","summary":"Answering questions from university admission exams (Gaokao in Chinese) is a challenging AI task since it requires effective representation to capture complicated semantic relations between questions and answers. In this work, we propose a hybrid neural model for deep question-answering task from history examinations. Our model employs a cooperative gated neural network to retrieve answers with the assistance of extra labels given by a neural turing machine labeler. Empirical study shows that the labeler works well with only a small training dataset and the gated mechanism is good at fetching the semantic representation of lengthy answers. Experiments on question answering demonstrate the proposed model obtains substantial performance gains over various neural model baselines in terms of multiple evaluation metrics. ","tags":null,"title":"One-shot Learning for Question-Answering in Gaokao History Challenge","type":"publication"},{"authors":["Zhuosheng Zhang","Jiangtong Li","Hai Zhao","Bingjie Tang"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"b14d77e9b82d0881f7e050b20e5ab30b","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2018-sjtu/","publishdate":"2018-01-01T00:00:00Z","relpermalink":"/publication/content/publication/zhang-2018-sjtu/","section":"publication","summary":"","tags":null,"title":"SJTU-NLP at SemEval-2018 Task 9: Neural Hypernym Discovery with Term Embeddings","type":"publication"},{"authors":["**Zhuosheng Zhang**","Yafang Huang","Hai Zhao"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"89e80f21e62e5d2dbddeb9a5f59dd8dc","permalink":"https://cooelf.github.io/publication/content/publication/zhang-2018-mrc/","publishdate":"2020-02-08T07:10:09.433319Z","relpermalink":"/publication/content/publication/zhang-2018-mrc/","section":"publication","summary":"Representation learning is the foundation of machine reading comprehension. In state-of-the-art models, deep learning methods broadly use word and character level representations. However, character is not naturally the minimal linguistic unit. In addition, with a simple concatenation of character and word embedding, previous models actually give suboptimal solution. In this paper, we propose to use subword rather than character for word embedding enhancement. We also empirically explore different augmentation strategies on subword-augmented embedding to enhance the cloze-style reading comprehension model reader. In detail, we present a reader that uses subword-level representation to augment word embedding with a short list to handle rare words effectively. A thorough examination is conducted to evaluate the comprehensive performance and generalization ability of the proposed reader. Experimental results show that the proposed approach helps the reader significantly outperform the state-of-the-art baselines on various public datasets. ","tags":null,"title":"Subword-augmented Embedding for Cloze Reading Comprehension","type":"publication"}]